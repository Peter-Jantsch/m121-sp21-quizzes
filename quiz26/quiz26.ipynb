{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OK\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('quiz32.ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this cell; just run it. \n",
    "# When you log-in please hit return (not shift + return) after typing in your email\n",
    "from client.api.notebook import *\n",
    "def new_save_notebook(self):\n",
    "    \"\"\" Saves the current notebook by\n",
    "        injecting JavaScript to save to .ipynb file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from IPython.display import display, Javascript\n",
    "    except ImportError:\n",
    "        log.warning(\"Could not import IPython Display Function\")\n",
    "        print(\"Make sure to save your notebook before sending it to OK!\")\n",
    "        return\n",
    "\n",
    "    if self.mode == \"jupyter\":\n",
    "        display(Javascript('IPython.notebook.save_checkpoint();'))\n",
    "        display(Javascript('IPython.notebook.save_notebook();'))\n",
    "    elif self.mode == \"jupyterlab\":\n",
    "        display(Javascript('document.querySelector(\\'[data-command=\"docmanager:save\"]\\').click();'))   \n",
    "\n",
    "    print('Saving notebook...', end=' ')\n",
    "\n",
    "    ipynbs = [path for path in self.assignment.src\n",
    "              if os.path.splitext(path)[1] == '.ipynb']\n",
    "    # Wait for first .ipynb to save\n",
    "    if ipynbs:\n",
    "        if wait_for_save(ipynbs[0]):\n",
    "            print(\"Saved '{}'.\".format(ipynbs[0]))\n",
    "        else:\n",
    "            log.warning(\"Timed out waiting for IPython save\")\n",
    "            print(\"Could not automatically save \\'{}\\'\".format(ipynbs[0]))\n",
    "            print(\"Make sure your notebook\"\n",
    "                  \" is correctly named and saved before submitting to OK!\".format(ipynbs[0]))\n",
    "            return False                \n",
    "    else:\n",
    "        print(\"No valid file sources found\")\n",
    "    return True\n",
    "\n",
    "def wait_for_save(filename, timeout=600):\n",
    "    \"\"\"Waits for FILENAME to update, waiting up to TIMEOUT seconds.\n",
    "    Returns True if a save was detected, and False otherwise.\n",
    "    \"\"\"\n",
    "    modification_time = os.path.getmtime(filename)\n",
    "    start_time = time.time()\n",
    "    while time.time() < start_time + timeout:\n",
    "        if (os.path.getmtime(filename) > modification_time and\n",
    "            os.path.getsize(filename) > 0):\n",
    "            return True\n",
    "        time.sleep(0.2)\n",
    "    return False\n",
    "\n",
    "Notebook.save_notebook = new_save_notebook\n",
    "import feedback\n",
    "import numpy as np\n",
    "from datascience import *\n",
    "ok = Notebook('quiz32.ok')\n",
    "_ = ok.auth(inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Assign answer_A to the statement that is *false*.**\n",
    "\n",
    "<ol>\n",
    "    <li>We take the root of the mean squared errors to make units more interpretable.</li> \n",
    "    <li>The regression line is the only straight line that minimizes mean squared error.</li>\n",
    "    <li>Errors can only be positive; it does not make sense to have negative values for error.</li>\n",
    "</ol>\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: qA\n",
    "manual: false\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_A = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok.grade(\"qA\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. True or False: The slope and intercept of the regression line can be found with the formulas `slope` = $r*\\frac{SD_y}{SD_x} $ and `intercept` = `average of y` - `slope` * `average of x` no matter what the shape of the scatter plot is.**\n",
    "\n",
    "1. `True`\n",
    "2. `False`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: qB\n",
    "manual: false\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_B = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok.grade(\"qB\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Consider the residual plot below. What does this say about the accuracy of the regression estimates for the data it describes? Assign answer_C to the true statement.**\n",
    "\n",
    "Tip: Read the *Regression Diagnostics* portion of [chapter 15.5](https://www.inferentialthinking.com/chapters/15/5/Visual_Diagnostics.html)\n",
    "\n",
    "<img src=\"residual_question.png\" width=300 height=300 />\n",
    "\n",
    "<ol>\n",
    "    <li>The regression estimates are equally accurate across the range of the predictor variable. Linear regression is a good way to model the relationshp between the two variables. </li>\n",
    "    <li>The regression estimates are not equally accurate across the range of the predictor variable. Linear regression is not a good way to model the relationshp between the two variables. </li>\n",
    "</ol>\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: qC\n",
    "manual: false\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_C = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok.grade(\"qC\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the submit cell to receive credit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
